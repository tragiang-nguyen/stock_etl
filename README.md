# D·ª± √Ån ETL Stock Pipeline

[![CI/CD Status](https://github.com/tragiang-nguyen/stock_etl/actions/workflows/ci-cd.yml/badge.svg)](https://github.com/tragiang-nguyen/stock_etl/actions/workflows/ci-cd.yml)

## T·ªïng Quan
D·ª± √°n n√†y l√† m·ªôt pipeline ETL (Extract, Transform, Load) end-to-end ƒë·∫ßy ƒë·ªß cho d·ªØ li·ªáu c·ªï phi·∫øu IBM, s·ª≠ d·ª•ng Apache Airflow ƒë·ªÉ ƒëi·ªÅu ph·ªëi, Hadoop HDFS/MapReduce/Hive cho x·ª≠ l√Ω batch, PySpark ƒë·ªÉ load d·ªØ li·ªáu, v√† PostgreSQL l√†m kho d·ªØ li·ªáu (DW). Pipeline scrape d·ªØ li·ªáu c·ªï phi·∫øu, l√†m s·∫°ch, t·ªïng h·ª£p (trung b√¨nh close theo timestamp), load v√†o DW, v√† t·ªëi ∆∞u h√≥a truy v·∫•n l√™n **53.55%** b·∫±ng index v√† partition (before: 0.282 ms, after: 0.131 ms).

C√°c t√≠nh nƒÉng ch√≠nh:
- X·ª≠ l√Ω batch c√≥ kh·∫£ nƒÉng m·ªü r·ªông v·ªõi h·ªá sinh th√°i Hadoop.
- ƒêi·ªÅu ph·ªëi t·ª± ƒë·ªông v·ªõi Airflow (retries=5, parallelism=32).
- T·ªëi ∆∞u h√≥a truy v·∫•n: Gi·∫£m th·ªùi gian execution t·ª´ 0.282 ms xu·ªëng 0.131 ms (c·∫£i thi·ªán 53.55%).
- CI/CD v·ªõi GitHub Actions (unit test + lint SQL).
- Unit test cho PySpark ETL (4/4 pass).

D·ª± √°n m√¥ ph·ªèng h·ªá th·ªëng ETL s·∫£n xu·∫•t (e.g., GCP BigQuery ETL v·ªõi Cloud Composer), ph√π h·ª£p cho v·ªã tr√≠ Junior Data Engineer.

## C√¥ng Ngh·ªá S·ª≠ D·ª•ng
- **ƒêi·ªÅu Ph·ªëi**: Apache Airflow 2.x (DAGs, BashOperator, PostgresOperator, PythonOperator).
- **X·ª≠ L√Ω Batch**: Hadoop HDFS, MapReduce (Java), Apache Hive (t·ªïng h·ª£p SQL).
- **X·ª≠ L√Ω D·ªØ Li·ªáu**: PySpark 3.5.1 (load JDBC v√†o PostgreSQL).
- **Kho D·ªØ Li·ªáu**: PostgreSQL 16 (DW v·ªõi index, c·ªôt generated cho partition).
- **Test**: pytest (unit test PySpark).
- **CI/CD**: GitHub Actions (test + lint tr√™n push/PR).
- **Kh√°c**: Python 3.12, Java 11 (cho MapReduce), Bash scripts.

## C·∫•u Tr√∫c D·ª± √Ån
```
stock_project/
‚îú‚îÄ‚îÄ .github/workflows/ci-cd.yml          # Pipeline CI/CD GitHub Actions
‚îú‚îÄ‚îÄ .gitignore                           # Lo·∫°i tr·ª´ binaries/logs
‚îú‚îÄ‚îÄ airflow/
‚îÇ   ‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ stock_dag.py                 # DAG Airflow (pipeline ETL)
‚îÇ   ‚îú‚îÄ‚îÄ airflow.cfg                      # Config Airflow
‚îÇ   ‚îî‚îÄ‚îÄ logs/                            # Logs Airflow (lo·∫°i tr·ª´ Git)
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ scrape_stock.py                  # Scrape d·ªØ li·ªáu c·ªï phi·∫øu (Python)
‚îÇ   ‚îú‚îÄ‚îÄ etl.py                           # PySpark ETL load v√†o PostgreSQL
‚îÇ   ‚îú‚îÄ‚îÄ hive_query.sql                   # SQL Hive (t·∫°o b·∫£ng, t·ªïng h·ª£p)
‚îÇ   ‚îî‚îÄ‚îÄ StockClean.java                  # Code MapReduce Java (l√†m s·∫°ch d·ªØ li·ªáu)
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ test_etl.py                      # Unit test PySpark (4/4 pass)
‚îú‚îÄ‚îÄ data/                                # D·ªØ li·ªáu m·∫´u c·ªï phi·∫øu (lo·∫°i tr·ª´ Git)
‚îú‚îÄ‚îÄ apache-hive-2.3.9-bin/               # Binary Hive (lo·∫°i tr·ª´)
‚îú‚îÄ‚îÄ hadoop/                              # Binary Hadoop (lo·∫°i tr·ª´)
‚îú‚îÄ‚îÄ spark/                               # Binary Spark (lo·∫°i tr·ª´)
‚îî‚îÄ‚îÄ README.md                            # File n√†y
```

## Y√™u C·∫ßu
- **OS**: Ubuntu 24.04 (WSL2 tr√™n Windows khuy·∫øn ngh·ªã).
- **Python**: 3.12 (venv cho Airflow).
- **Java**: OpenJDK 11 (cho Hadoop/Hive/MapReduce).
- **PostgreSQL**: 16 (user `datauser` / password `password`, DB `airflow`).
- **Hadoop**: 3.x (HDFS, YARN, single-node).
- **Hive**: 2.3.9 (Metastore service).
- **Airflow**: 2.x (v·ªõi provider postgres).
- **Git**: ƒê√£ c√†i (clone repo).

RAM: √çt nh·∫•t 8GB (cho Spark/Hadoop).

## C√†i ƒê·∫∑t
1. **Clone Repo**:
   ```
   git clone https://github.com/tragiang-nguyen/stock_etl.git
   cd stock_etl
   ```

2. **C√†i Java**:
   ```
   sudo apt update && sudo apt install openjdk-11-jdk
   export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
   echo 'export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64' >> ~/.bashrc
   source ~/.bashrc
   ```

3. **C√†i Hadoop (Single-Node)**:
   - Download binary Hadoop 3.x t·ª´ [hadoop.apache.org](https://hadoop.apache.org/releases.html), gi·∫£i n√©n v√†o `./hadoop`.
   - S·ª≠a `./hadoop/etc/hadoop/core-site.xml` (d·ª±a tr√™n config th·ª±c t·∫ø c·ªßa b·∫°n):
     ```
     <?xml version="1.0" encoding="UTF-8"?>
     <configuration>
         <property>
             <name>fs.defaultFS</name>
             <value>hdfs://localhost:9000</value>
         </property>
         <property>
             <name>hadoop.tmp.dir</name>
             <value>/home/user/hadoop-tmp</value>
         </property>
     </configuration>
     ```
   - S·ª≠a `./hadoop/etc/hadoop/hdfs-site.xml` (d·ª±a tr√™n config th·ª±c t·∫ø):
     ```
     <?xml version="1.0" encoding="UTF-8"?>
     <configuration>
         <property>
             <name>dfs.replication</name>
             <value>1</value>
         </property>
         <property>
             <name>dfs.namenode.name.dir</name>
             <value>file:${hadoop.tmp.dir}/dfs/name</value>
         </property>
         <property>
             <name>dfs.datanode.data.dir</name>
             <value>file:${hadoop.tmp.dir}/dfs/data</value>
         </property>
     </configuration>
     ```
   - Format HDFS v√† start (d·ª±a tr√™n thao t√°c th·ª±c t·∫ø):
     ```
     $HADOOP_HOME/bin/hdfs namenode -format
     $HADOOP_HOME/sbin/start-dfs.sh
     $HADOOP_HOME/sbin/start-yarn.sh
     ```
     - Ki·ªÉm tra: `jps` (NameNode, DataNode, SecondaryNameNode, ResourceManager, NodeManager).

4. **C√†i Hive**:
   - Download binary Hive 2.3.9 t·ª´ [hive.apache.org](https://hive.apache.org/downloads.html), gi·∫£i n√©n v√†o `./apache-hive-2.3.9-bin`.
   - S·ª≠a `./apache-hive-2.3.9-bin/conf/hive-site.xml` (d·ª±a tr√™n config th·ª±c t·∫ø c·ªßa b·∫°n):
     ```
     <?xml version="1.0"?>
     <configuration>
         <property>
             <name>javax.jdo.option.ConnectionURL</name>
             <value>jdbc:postgresql://localhost:5432/airflow</value>
         </property>
         <property>
             <name>javax.jdo.option.ConnectionDriverName</name>
             <value>org.postgresql.Driver</value>
         </property>
         <property>
             <name>javax.jdo.option.ConnectionUserName</name>
             <value>datauser</value>
         </property>
         <property>
             <name>javax.jdo.option.ConnectionPassword</name>
             <value>password</value>
         </property>
         <property>
             <name>hive.metastore.schema.verification</name>
             <value>false</value>
         </property>
         <property>
           <name>hive.metastore.uris</name>
           <value>thrift://localhost:9083</value>
         </property>
     </configuration>
     ```
   - Init schema (l·∫ßn ƒë·∫ßu):
     ```
     $HIVE_HOME/bin/schematool -dbType postgres -initSchema
     ```
   - Start Metastore (d·ª±a tr√™n thao t√°c th·ª±c t·∫ø):
     ```
     nohup $HIVE_HOME/bin/hive --service metastore > hive-metastore.log 2>&1 &
     ```
     - Ki·ªÉm tra: `ps aux | grep metastore` (RunJar process), `tail -f hive-metastore.log` (Starting Hive Metastore Server).

5. **C√†i PostgreSQL**:
   - C√†i: `sudo apt install postgresql postgresql-contrib`
   - Restart (d·ª±a tr√™n thao t√°c th·ª±c t·∫ø):
     ```
     sudo service postgresql restart
     sudo service postgresql status
     ```
   - T·∫°o DB/user:
     ```
     sudo -u postgres psql
     CREATE USER datauser WITH PASSWORD 'password';
     CREATE DATABASE airflow OWNER datauser;
     \q
     ```

6. **C√†i Airflow**:
   - T·∫°o venv: `python3 -m venv /home/user/airflow_env && source /home/user/airflow_env/bin/airflow activate`
   - C√†i: `pip install apache-airflow[postgres]`
   - Config `airflow.cfg`: executor = SequentialExecutor, sql_alchemy_conn = postgresql://datauser:password@localhost/airflow
   - Init DB: `airflow db init`
   - Start (d·ª±a tr√™n thao t√°c th·ª±c t·∫ø):
     ```
     sudo -E /home/user/airflow_env/bin/airflow webserver -p 8080 > airflow-webserver.log 2>&1 &
     nohup /home/user/airflow_env/bin/airflow scheduler > airflow-scheduler.log 2>&1 &
     ```
     - Ki·ªÉm tra: `ps aux | grep airflow` (gunicorn workers, scheduler), `tail -f airflow-scheduler.log` (Starting the scheduler).

## Ch·∫°y D·ª± √Ån
1. **Start Services** (Hadoop, Hive, PostgreSQL, Airflow ‚Äì xem C√†i ƒê·∫∑t).
   - Hadoop: `$HADOOP_HOME/sbin/start-dfs.sh && $HADOOP_HOME/sbin/start-yarn.sh`
   - Hive: `nohup $HIVE_HOME/bin/hive --service metastore > hive-metastore.log 2>&1 &`
   - PostgreSQL: `sudo service postgresql restart`
   - Airflow: `sudo -E /home/user/airflow_env/bin/airflow webserver -p 8080 > airflow-webserver.log 2>&1 &` v√† `nohup /home/user/airflow_env/bin/airflow scheduler > airflow-scheduler.log 2>&1 &`
2. **Trigger DAG**:
   ```
   /home/user/airflow_env/bin/airflow dags trigger stock_etl
   ```
   - Theo d√µi: http://localhost:8080 (login admin/admin), DAGs ‚Üí stock_etl ‚Üí Graph View.
   - Th·ªùi gian ch·∫°y: ~5-10 ph√∫t (scrape ‚Üí load ‚Üí optimize).

3. **Ki·ªÉm Tra K·∫øt Qu·∫£**:
   - D·ªØ li·ªáu loaded:
     ```
     psql -U datauser -d airflow -h localhost -c "SELECT COUNT(*) FROM stock_data;"
     ```
     - K·∫øt qu·∫£ mong ƒë·ª£i: ~4380 rows.
   - Hi·ªáu su·∫•t truy v·∫•n:
     ```
     psql -U datauser -d airflow -h localhost -c "EXPLAIN ANALYZE SELECT * FROM stock_data WHERE tstamp > '2025-10-01';"
     ```
     - K·∫øt qu·∫£ mong ƒë·ª£i: Execution Time ~0.131 ms (Index Scan).

## Test D·ª± √Ån
1. **Unit Test (PySpark ETL)**:
   ```
   source /home/user/stock_test_env/bin/activate  # Venv v·ªõi pyspark 3.5.1
   cd tests
   python -m pytest test_etl.py -v
   deactivate
   ```
   - K·∫øt qu·∫£ mong ƒë·ª£i: 4/4 PASSED (schema, cast, filter/order, mock JDBC).

2. **Ki·ªÉm Tra % Gi·∫£m Hi·ªáu Su·∫•t** (T·ªëi ∆Øu H√≥a Truy V·∫•n):
   - Theo d√µi log scheduler sau trigger:
     ```
     tail -f airflow/logs/scheduler.log | grep -i "BEFORE_TIME\|AFTER_TIME\|Ph·∫ßn trƒÉm gi·∫£m"
     ```
     - K·∫øt qu·∫£ mong ƒë·ª£i: "BEFORE_TIME: 0.282 ms", "AFTER_TIME: 0.131 ms", "Ph·∫ßn trƒÉm gi·∫£m: 53.55%".

3. **Test CI/CD** (GitHub Actions):
   - Push code ‚Üí Tab Actions ‚Üí "CI/CD ETL Stock Pipeline" t·ª± ch·∫°y (test + lint pass, checkmark xanh).

## X·ª≠ L√Ω L·ªói
- **Airflow kh√¥ng start**: Ki·ªÉm tra log (`tail -f airflow/logs/scheduler.log`), chown airflow dir (`sudo chown -R user:user airflow`).
- **HDFS/Hive l·ªói**: Start services (`start-dfs.sh`, hive --service metastore`), ki·ªÉm tra jps (`jps | grep NameNode`).
- **PostgreSQL conn fail**: Restart (`sudo service postgresql restart`), ki·ªÉm tra user/DB (`psql -U datauser -d airflow -h localhost`).
- **PySpark test fail**: Reinstall venv (`pip install pyspark==3.5.1 pytest`).
- **% Gi·∫£m kh√¥ng 53%**: Ch·∫°y sau DAG success (data/index update), dao ƒë·ªông t√πy load.

## K·∫øt Qu·∫£
- **D·ªØ Li·ªáu**: ~4380 rows loaded v√†o PostgreSQL DW.
- **T·ªëi ∆Øu H√≥a**: Th·ªùi gian truy v·∫•n gi·∫£m 53.55% (before 0.282 ms, after 0.131 ms qua index/partition).
- **CI/CD**: GitHub Actions xanh (4 test pass, lint SQL success).

---
N·∫øu h·ªç cung c·∫•p output/screenshot kh·ªõp (~4380 rows, ~53% gi·∫£m, 4 tests pass, green Actions), ch·ª©ng t·ªè h·ªç ch·∫°y th·ª±c t·∫ø (kh√¥ng fake). N·∫øu kh√¥ng, h·ªèi "K·∫πt ·ªü b∆∞·ªõc n√†o?" ƒë·ªÉ h∆∞·ªõng d·∫´n.

D·ª± √°n gi·ªù **s·∫µn s√†ng** ‚Äì apply job ƒëi! üöÄ
